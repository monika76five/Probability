\documentclass[slidestop,compress,mathserif]{beamer}
%\documentclass[slidestop,compress,mathserif,handout]{beamer}

%\documentclass[xcolor=dvipsnames,handout]{beamer}
%\documentclass[xcolor=dvipsnames]{beamer}

%\documentclass[handout]{beamer}

%%% To get rid of solutions on handouts:
\newcommand{\soln}[1]{\textit{\textcolor{darkGray}{#1}}}				% For slides
%\newcommand{\soln}[1]{ }	% For handouts

% to get pausing to work properly on slides
\newcommand{\hide}[1]{#1}	% For slides
%\newcommand{\hide}[1]{ }	% For handouts
\usepackage{tikz}


\input{../LectureStyle.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[Chapter 6 part 2]{Chapter 6 part 2}
\subtitle{Jointly Distributed Random Variables}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\author[Jingchen (Monika) Hu] % (optional, use only with lots of authors)
{Jingchen (Monika) Hu}
% - Give the names in the same order as the appear in the paper.
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[Vassar] % (optional, but mostly needed)
{Vassar College}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date[MATH 241] % (optional, should be abbreviation of conference name)
{MATH 241}
% - Either use conference name or its abbreviation.
% - Not really informative to the audience, more for people (including
%   yourself) who are reading the slides online

\subject{MATH 241}
% This is only inserted into the PDF information catalog. Can be left
% out.



% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command:

%\beamerdefaultoverlayspecification{<+->}



\begin{document}

%\begin{frame}%[plain]
%\includegraphics[width = \textwidth]{figures/2015DukeNCAA}
%\end{frame}
%
%{ % all template changes are local to this group.
%\addtocounter{framenumber}{-1}
%    \setbeamertemplate{navigation symbols}{}
%    \begin{frame}[plain]
%        \begin{tikzpicture}[remember picture,overlay]
%            \node[at=(current page.center)] {
%                \includegraphics[width=1.25\paperwidth]{figures/2015DukeNCAA}
%            };
%        \end{tikzpicture}
%     \end{frame}
%}

%%%%%%%%%%%%%%%%%%%%%

% Title Page

\begin{frame}%[plain]
\titlepage
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%
%\addtocounter{framenumber}{-1}
%
%\begin{frame}\frametitle{Annoucement}
%
%\begin{itemize}
%%\item HW7: \red{due now!}
%\item HW8: \red{due Tuesday, Nov 20th}
%
%
%\vspace{0.5cm}
%\item Course evaluation open tomorrow.
%  \begin{itemize}
%  \item Activate between 11/14 - 12/3.
%  \item If response rate $\geq 80\%$, drop the lowest quiz.
%  \end{itemize}
%
%\vspace{0.5cm}
%\item Next quiz: Tuesday, Nov 18th\\
%  \begin{itemize}
%  \item Topic: function of a continuous RV, i.e., find $f_Y(y)$ where $Y = g(X)$.
%  %\item To prepare: do homework questions in Chapter 5: 37, 39, 40, TE29
%  \end{itemize}
%
%
%%\item Midterm: Tuesday, Feb 25th
%%\begin{itemize}
%%\item Close book, in class exam (75 min)
%%\item ONE page cheat sheet {\bf made by yourself} (A4 size)
%%\item Calculators are allowed, but not cell phones, tablets or laptops
%%\end{itemize}
%
%\end{itemize}
%
%
%\end{frame}
%

%
%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Outline}
%\tableofcontents[hideallsubsections,pausections]
\tableofcontents[hideallsubsections]
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}\frametitle{Recap}
%
%\end{frame}





%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}{Outline}
%%\tableofcontents[hideallsubsections,pausections]
%\tableofcontents[hideallsubsections]
%\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sums of independent random variables}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Sums of continuous random variables}

If $X,Y$ have a joint density $f(x,y)$, then $X+Y$ has the following density
\begin{align*}
f_{X+Y}(z) &= \int_{-\infty}^\infty f(x,z-x)~dx \\
           &= \int_{-\infty}^\infty f(z-y,y)~dy \\
\end{align*}

\vspace{-0.3cm} 
If $X$ and $Y$ are independent, we can use the convolution formula
\begin{align*}
f_{X+Y}(z) &= \int_{-\infty}^\infty f_X(x)f_Y(z-x)~dx \\
           &= \int_{-\infty}^\infty f_X(z-y)f_Y(y)~dy \\
\end{align*}

\vspace{-0.3cm} 
Cdf $F_{X+Y}$ is called the \hl{convolution} of the distributions $F_X$ and $F_Y$.


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Why?}

\begin{align*}
F_{X+Y}(z) & = \iint_{x+y\leq z} f_{X, Y}(x,y)~dy ~dx\\
 & = \int_{-\infty}^\infty\int_{-\infty}^{z-x} f_{X, Y}(x,y)~dy ~dx\\
f_{X+Y}(z) & =  \frac{d}{dz}F_{X+Y}(z)\\
& = \int_{-\infty}^\infty\left\{\frac{d}{dz}\int_{-\infty}^{z-x} f_{X, Y}(x,y)~dy \right\}~dx\\
& = \int_{-\infty}^\infty f_{X, Y}(x,z-x)~dx
\end{align*}

Textbook page 239 gives a derivation when $X$ and $Y$ are independent. 

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}%\frametitle{Sum of independent Exponentials (same $\lambda$) }
\cl{Suppose $X, Y \stackrel{\text{ind}}{\sim} \text{Exp}(\lambda)$. What distribution does $X+Y$ have? Note that the pdf of exponential of $X \sim \text{Exp}(\lambda)$ is: $f(x) = \lambda e^{-\lambda x}, x > 0$.}

%\pause



\uncover<2->{Range of $X + Y$ is $(0, \infty)$. For any $z > 0$,}
\begin{align*}
\uncover<3->{f_{X+Y}(z) &= \int_{-\infty}^\infty f_X(x)f_Y(z-x)~dx \\}
\uncover<4->{  &= \int_{{\color{red} 0}}^{{\color{red} z}} \lambda e^{-\lambda x} \lambda e^{-\lambda (z-x)} ~dx \\}
\uncover<5->{  &= \int_{0}^z \lambda^2 e^{-\lambda z}~dx \\}
\uncover<6->{   &= \left. \lambda^2 x e^{-\lambda z} \right|_0^z } \uncover<7->{ = \lambda^2 z e^{-\lambda z}  \\}
\uncover<8->{     f_{X+Y}(z)      &= \begin{cases}
              \lambda^2 z e^{-\lambda z} & \text{if $z>0$} \\
              0 & \text{otherwise}
              \end{cases} }
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{frame}\frametitle{Sum of independent Gammas (same $\lambda$) }
%\cl{Suppose $X \sim \text{G}(\alpha_1, \lambda)$, and $Y \sim \text{G}(\alpha_2, \lambda)$independently. What distribution does $X+Y$ have?}
%
%\pause
%
%Range of $X+Y$ is $(0, \infty)$. For any $z > 0$,
%\begin{align*}
%f_{X+Y}(z) &= \int_{-\infty}^\infty f_X(x)f_Y(z-x)~dx \\
%           &\propto \int_{0}^z x^{s-1} e^{-\lambda x} (z-x)^{t-1} e^{-\lambda (z-x)} ~dx \\
%\uncover<3->{
%           &= e^{-\lambda z} \int_{0}^z x^{s-1} (z-x)^{t-1}  ~dx \\}
%\uncover<4->{
%           &= z^{s + t -1} e^{-\lambda z} \int_{0}^1 \left(\frac{x}{z}\right)^{s-1} \left(1-\frac{x}{z}\right)^{t-1}  ~d \frac{x}{z} \\
%           &= z^{s + t -1} e^{-\lambda z}  B(s,t) \\}
%\uncover<5->{
%           & \propto z^{s + t -1} e^{-\lambda z}}
%\end{align*}
%\uncover<6->{
%Therefore, $X+Y \sim \text{Gamma}(s+t,\lambda)$.}
%
%
%\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Sum of independent random variables }

Random variables $X$ and $Y$ are independent, then
\begin{center}
\begin{tabular}{lll}
\hline
$X$\rule{0pt}{3ex}							& $Y$ 						& $X + Y$\\
\hline
N$(\mu_1, \sigma^2_1)$ \rule{0pt}{3ex}		& N$(\mu_2, \sigma^2_2)$	&N$(\mu_1 + \mu_2, \sigma^2_1+\sigma^2_2)$\\
&&\\
%G$(\alpha_1, \lambda)$	\rule{0pt}{3ex}		& G$(\alpha_2, \lambda)$	& \uncover<3->{G$(\alpha_1 + \alpha_2, \lambda)$}\\
%&&\\
Poi$(\lambda_1)$	\rule{0pt}{3ex}			& Poi$(\lambda_2)$			& Poi$(\lambda_1 + \lambda_2)$\\
&&\\
Bin$(n_1, p)$	\rule{0pt}{3ex}				& Bin$(n_2, p)$			& Bin$(n_1 + n_2, p)$\\
\hline
\end{tabular}
\end{center}



\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{frame}\frametitle{Sum of independent Chi-squared distributions}
%\cl{Suppose $X \sim \chi^2(n_1)$, and $Y \sim \chi^2(n_2)$ independently. What distribution does $X+Y$ have?}
%\pause
%Hint: recall that $\chi^2(n) = G(n/2, 1/2)$.
%\pause
%\[ X \sim \text{G}\left( \frac{n_1}{2}, \frac{1}{2} \right), Y \sim \text{G}\left( \frac{n_2}{2}, \frac{1}{2}\right)\]
%\[ X + Y \sim \text{G}\left( \frac{n_1 + n_2}{2}, \frac{1}{2}\right) \sim \chi^2\left( n_1 + n_2\right)\]
%
%\end{frame}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}%\frametitle{Difference of independent Normal random variables}
\cl{Suppose $X \sim \text{N}(\mu_1, \sigma^2_1)$, and $Y \sim \text{N}(\mu_2, \sigma^2_2)$ independently. What distribution does $X-Y$ have?}
\pause
Hint: find the distribution of $W = -Y$ first. \\
Since $g(y) = -y$ is monotonic and differentiable on $\mathbb{R}$,
\[
f_W(w) = f_Y(y) \left| \frac{dy}{dw} \right| \uncover<3->{= \frac{1}{\sqrt{2\pi \sigma^2_2}} e^{-\frac{(-w - \mu_2)^2}{2\sigma^2_2}} }
\]
 \uncover<3->{
\[ W \sim \text{N}(-\mu_2, \sigma_2^2)\]
Since $X$ and $W$ are also independent,
\[ X - Y = X + W  \sim \text{N}\left( \mu_1 - \mu_2, \sigma^2_1+\sigma^2_2 \right)\]
}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Recap}

Random variables $X$ and $Y$ are independent if any real sets $A, B \subset \mathbb{R}$,
\[P(X \in A, Y \in B) = P(X \in A) P(Y \in B)\]

Random variables $X$ and $Y$ are independent {\bf if and only if}
\begin{itemize}
\item Cdf: for any $x, y \in \mathbb{R}$
\[F_{X, Y}(x, y) = F_X(x) F_Y(y)\]
\item  For any $x, y \in \mathbb{R}$, the pmf / pdf
\[f_{X, Y}(x, y) = f_X(x) f_Y(y)\]
\end{itemize}

If $X$ and $Y$ are independent continuous random variables, then
\begin{align*}
f_{X+Y}(z) &= \int_{-\infty}^\infty f_X(x)f_Y(z-x)~dx \\
\end{align*}


\end{frame}



%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}{Outline}
%%\tableofcontents[hideallsubsections,pausections]
%\tableofcontents[hideallsubsections]
%\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conditional distributions: discrete case}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Discrete conditional distributions}
\begin{itemize}
\item Recall that for any two events $E$ and $F$, the conditional probability of $E$ given $F$ is defined by
$$P(E|F) = \frac{P(E\cap F)}{P(F)}$$
provided that $P(F) > 0$.
\vspace{4mm}
\pause
\item If $X$ and $Y$ are discrete random variables, we define the conditional probability mass function (pmf) of $X$ given $Y=y$ by
$$p_{X|Y}(x|y) = P\{X = x|Y = y\} = \frac{P\{X = x, Y = y\}}{P\{Y = y\}} = \frac{p(x, y)}{p_Y(y)}$$
for all values of $y$ such that $p_Y(y) > 0$.
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Discrete conditional distributions}
\begin{itemize}
\item Similarly for the conditional probability density function (cdf), we define it by
$$F_{X|Y}(x|y) = P\{X \leq x|Y \leq y\} = \sum_{x \leq a}p_{X|Y}(a|y)$$
\vspace{4mm}
\item If $X$ is independent of $Y$, then
\begin{eqnarray*}
p_{X|Y}(x|y) &=& P\{X = x|Y = y\}\\
&=&\frac{P\{X = x, Y = y\}}{P\{Y = y\}} \\
&=&\frac{P\{X = x\}P\{Y = y\}}{P\{Y = y\}}\\
&=&P\{X = x\}
\end{eqnarray*}
\end{itemize}
\end{frame}



\begin{frame}%\frametitle{Discrete conditional distributions}
\cl{If $X$ and $Y$ are independent Poisson random variables with respective parameters $\lambda_1$ and $\lambda_2$, calculate the conditional distribution of $X$ given that $X + Y = n$.}
\pause
{\small{
\begin{itemize}
\item $P\{X = k | X + Y = n\} = \frac{P\{X = k, X + Y = n\}}{P\{X + Y = n\}} = \frac{P\{X = k, Y = n - k\}}{P\{X + Y = n\}} \underset{ind}{=} \frac{P\{X = k\}P\{ Y = n - k\}}{P\{X + Y = n\}}$
\pause
\item We know that $X + Y \sim P(\lambda_1 + \lambda_2)$ (sum of Poisson)
\begin{eqnarray*}
P\{X = k | X + Y = n\} &=& \frac{e^{-\lambda_1}\lambda_1^k}{k!}\frac{e^{-\lambda_2}\lambda_2^{n-k}}{(n-k)!}[\frac{e^{-(\lambda_1+\lambda_2)}(\lambda_1+\lambda_2)^n}{n!}]^{-1}\\
    &=&
    \frac{n!}{(n-k)!k!}\frac{\lambda_1^k\lambda_2^{(n-k)}}{(\lambda_1+\lambda_2)^n}\\
    &=&
    {n \choose k} \left(\frac{\lambda_1}{\lambda_1+\lambda_2}\right)^k\left(\frac{\lambda_2}{\lambda_1+\lambda_2}\right)^{(n-k)}
\end{eqnarray*}
\item That is, the conditional distribution of $X$ given $X + Y = n$ is a $Bin(n, \lambda_1/(\lambda_1+\lambda_2))$
\end{itemize}
}}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conditional distributions: continuous case}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Continuous conditional distributions}
\begin{itemize}
\item With joint probability function of $X$ and $Y$ as $f(x, y)$, the conditional pdf of $X$ given that $Y = y$ (for $f_Y(y) > 0$) is defined by
$$f_{X|Y}(x|y) = \frac{f(x, y)}{f_Y(y)}$$
note that the event $\{Y = y\}$ has probability $0$; we just use it for conditioning.
\item If $X$ and $Y$ are jointly continuous, then for any set $A$
$$P\{X \in A|Y = y\} = \int_A f_{X|Y}(x|y)dx$$
This is defining conditional probabilities of events associated with one random variable when we are given the value of a second random variable.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Continuous conditional distributions}
\begin{itemize}
\item By setting $A = (-\infty, a)$, we can define the cdf of $X$ given that $Y = y$ by
$$F_{X|Y}(a|y) = P\{X \leq a|Y = y\} = \int_{-\infty}^a f_{X|Y}(x|y)dx$$
\item And if $X$ and $Y$ are independent continuous random variables,
$$f_{X|Y}(x|y) = \frac{f(x, y)}{f_Y(y)} = \frac{f_X(x)f_Y(y)}{f_Y(y)} = f_X(x)$$
This is the unconditional density of $X$.
\end{itemize}
\end{frame}

\begin{frame}%\frametitle{Continuous conditional distributions}
\cl{Suppose the joint density of $X$ and $Y$ is given by
\[ f(x,y) = \begin{cases}
    \frac{e^{-x/y}e^{-y}}{y} 		 & \text{ for } 0 < x < \infty, 0 < y < \infty \\
    0         & \text{otherwise}
\end{cases}
 \]
Find $P\{X > 1 \mid Y = y\}$.}
\pause
{\small{
\begin{multicols}{2}
First, obtain the conditional density of $X$ given $Y = y$,
\begin{eqnarray*}
f_{X \mid Y}(x \mid y) &=& \frac{f(x, y)}{f_Y(y)}\\
&=&
\frac{e^{-x/y}e^{-y}/y}{e^{-y}\int_0^{\infty}(1/y)e^{-x/y}dx}\\
&=&
\frac{1}{y}e^{-x/y}
\end{eqnarray*}
\pause
Hence,
\begin{eqnarray*}
P\{X > 1 \mid Y = y\} &=& \int_1^{\infty}\frac{1}{y}e^{-x/y}dx\\
&=&
-e^{-x/y}|_1^{\infty}\\
&=&
e^{-1/y}
\end{eqnarray*}
\end{multicols}
}}

\end{frame}
\end{document}
